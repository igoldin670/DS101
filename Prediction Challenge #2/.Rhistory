getwd()
train <- read.csv("Prediction2025-2Train.csv")
train[train$Outcome == "Success",]
setwd('/home/isaac/Code/School/DS101/Prediction Challenge #2')
train[train$Outcome == "Success",]
train[train$Outcome == "Failure",]
train <- read.csv("Prediction2025-2Train.csv")
train[train$Outcome == "Success",]
train[train$Outcome == "Failure",]
train$predicted <- rep("Failure", nrow(train))
train[train$Groom_Edu == train$Bride_Edu & train$Outcome == "Success", ]
train$predicted[train$Groom_Edu == train$Bride_Edu] <- "Success"
train[train$Outcome != train$predicted & train$Outcome == "Success",]
train[train$Outcome != train$predicted & train$Outcome == "Failure",]
train$predicted[train$Groom_MB == "ENFJ" & train$Bride_MB == "INTP"] <- "Success"
train$predicted[train$Groom_MB == "ENFJ" & train$Bride_MB == "ISFJ"] <- "Failure"
train$predicted[train$Groom_MB == "ENFP" & train$Bride_MB == "ENFP"] <- "Success"
train$predicted[train$Groom_Age %% train$Bride_Age >= 7] <- "Success"
accuracy <- sum(train$Outcome == train$predicted) / nrow(train)
print(accuracy)
library(rpart)
library(rpart.plot)
# Load the dataset
data <- read.csv("Prediction2025-2Train.csv")
test <- read.csv("Prediction2025-2Train.csv")
# Encode categorical variables in R
data$Groom_MB <- as.numeric(factor(data$Groom_MB))
data$Bride_MB <- as.numeric(factor(data$Bride_MB))
data$Groom_Edu <- as.numeric(factor(data$Groom_Edu))
data$Bride_Edu <- as.numeric(factor(data$Bride_Edu))
data$Outcome <- ifelse(data$Outcome == "Success", 1, 0)
# Train a more flexible decision tree
tree <- rpart(Outcome ~ ., data = data, method = "class",
cp = 0.001,     # Reduce pruning for better learning
minsplit = 5,   # Allow smaller nodes to split
minbucket = 2,  # Reduce required instances per leaf
maxdepth = 10)  # Allow deeper trees for better accuracy
# Predictions
pred <- predict(tree, data, type = "class")
data$predict <- pred
# Evaluate accuracy
test$predict <- ifelse(data$predict == 1, "Success", "Failure")
accuracy <- mean(test$Outcome == test$predict)
print(paste("Model Accuracy:", round(accuracy, 4)))
test[test$predict != test$Outcome,]
# Visualize the improved tree
rpart.plot(tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Improved Decision Tree")
library(rpart)
library(rpart.plot)
# Load the dataset
data <- read.csv("Prediction2025-2Train.csv")
# Encode categorical variables in R
data$Groom_MB <- as.numeric(factor(data$Groom_MB))
data$Bride_MB <- as.numeric(factor(data$Bride_MB))
data$Groom_Edu <- as.numeric(factor(data$Groom_Edu))
data$Bride_Edu <- as.numeric(factor(data$Bride_Edu))
data$Outcome <- ifelse(data$Outcome == "Success", 1, 0)
# Train a more flexible decision tree
tree <- rpart(Outcome ~ ., data = data, method = "class",
cp = 0.001,     # Reduce pruning for better learning
minsplit = 5,   # Allow smaller nodes to split
minbucket = 2,  # Reduce required instances per leaf
maxdepth = 10)  # Allow deeper trees for better accuracy
# Predictions
pred <- predict(tree, data, type = "class")
data$predict <- pred
# Evaluate accuracy
test$predict <- ifelse(data$predict == 1, "Success", "Failure")
accuracy <- mean(data$Outcome == data$predict)
print(paste("Model Accuracy:", round(accuracy, 4)))
# Visualize the improved tree
rpart.plot(tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Improved Decision Tree")
library(rpart)
library(rpart.plot)
# Load the dataset
data <- read.csv("Prediction2025-2Train.csv")
# Encode categorical variables in R
data$Groom_MB <- as.numeric(factor(data$Groom_MB))
data$Bride_MB <- as.numeric(factor(data$Bride_MB))
data$Groom_Edu <- as.numeric(factor(data$Groom_Edu))
data$Bride_Edu <- as.numeric(factor(data$Bride_Edu))
data$Outcome <- ifelse(data$Outcome == "Success", 1, 0)
# Train a more flexible decision tree
tree <- rpart(Outcome ~ ., data = data, method = "class",
cp = 0.001,     # Reduce pruning for better learning
minsplit = 5,   # Allow smaller nodes to split
minbucket = 2,  # Reduce required instances per leaf
maxdepth = 10)  # Allow deeper trees for better accuracy
# Predictions
pred <- predict(tree, data, type = "class")
data$predict <- pred
# Evaluate accuracy
test$predict <- ifelse(data$predict == 1, "Success", "Failure")
accuracy <- mean(data$Outcome == data$predict)
print(paste("Model Accuracy:", round(accuracy, 4)))
# Visualize the improved tree
rpart.plot(tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Improved Decision Tree")
library(rpart)
library(rpart.plot)
# Load datasets
train_data <- read.csv("Prediction2025-2Train.csv")
test_data <- read.csv("Prediction2025-2TestStud.csv")
# Encode categorical variables
train_data$Groom_MB <- as.numeric(factor(train_data$Groom_MB))
train_data$Bride_MB <- as.numeric(factor(train_data$Bride_MB))
train_data$Groom_Edu <- as.numeric(factor(train_data$Groom_Edu))
train_data$Bride_Edu <- as.numeric(factor(train_data$Bride_Edu))
train_data$Outcome <- ifelse(train_data$Outcome == "Success", 1, 0)
test_data$Groom_MB <- as.numeric(factor(test_data$Groom_MB, levels = levels(factor(train_data$Groom_MB))))
test_data$Bride_MB <- as.numeric(factor(test_data$Bride_MB, levels = levels(factor(train_data$Bride_MB))))
test_data$Groom_Edu <- as.numeric(factor(test_data$Groom_Edu, levels = levels(factor(train_data$Groom_Edu))))
test_data$Bride_Edu <- as.numeric(factor(test_data$Bride_Edu, levels = levels(factor(train_data$Bride_Edu))))
# Train the decision tree
tree <- rpart(Outcome ~ ., data = train_data, method = "class",
cp = 0.001, minsplit = 5, minbucket = 2, maxdepth = 10)
# Predictions on test data
test_pred <- predict(tree, test_data, type = "class")
# Store predictions
test_data$Outcome <- ifelse(test_pred == 1, "Success", "Failure")
# Save results
write.csv(test_data, "Prediction2025-2submission.csv", row.names = FALSE)
# Note: Prediction performance might not be great because midterms took priority
# Load the test dataset
test_data <- read.csv("Prediction2025-2TestStud.csv")
# Encode categorical variables in the test data
# (ensure that encoding is consistent with the training data)
test_data$Groom_MB <- as.numeric(factor(test_data$Groom_MB))
test_data$Bride_MB <- as.numeric(factor(test_data$Bride_MB))
test_data$Groom_Edu <- as.numeric(factor(test_data$Groom_Edu))
test_data$Bride_Edu <- as.numeric(factor(test_data$Bride_Edu))
# Use the trained decision tree to predict outcomes for the test data
test_pred <- predict(tree, newdata = test_data, type = "class")
# Convert numeric predictions back to character labels if needed
# Here, assuming 1 represents "Success" and 0 represents "Failure"
test_data$Outcome <- ifelse(test_pred == 1, "Success", "Failure")
# Write the predictions to a CSV file for submission
write.csv(test_data, file = "submission.csv", row.names = FALSE)
# Optional: Print a message to confirm that the submission file has been created.
print("Submission file 'submission.csv' has been created successfully.")
library(rpart)
library(rpart.plot)
# Load the dataset
data <- read.csv("Prediction2025-2Train.csv")
# Encode categorical variables in R
data$Groom_MB <- as.numeric(factor(data$Groom_MB))
data$Bride_MB <- as.numeric(factor(data$Bride_MB))
data$Groom_Edu <- as.numeric(factor(data$Groom_Edu))
data$Bride_Edu <- as.numeric(factor(data$Bride_Edu))
data$Outcome <- ifelse(data$Outcome == "Success", 1, 0)
# Train a more flexible decision tree
tree <- rpart(Outcome ~ ., data = data, method = "class",
cp = 0.001,     # Reduce pruning for better learning
minsplit = 5,   # Allow smaller nodes to split
minbucket = 2,  # Reduce required instances per leaf
maxdepth = 10)  # Allow deeper trees for better accuracy
# Predictions
pred <- predict(tree, data, type = "class")
data$predict <- pred
# Evaluate accuracy
test$predict <- ifelse(data$predict == 1, "Success", "Failure")
accuracy <- mean(data$Outcome == data$predict)
print(paste("Model Accuracy:", round(accuracy, 4)))
# Visualize the improved tree
rpart.plot(tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Improved Decision Tree")
# Note: Prediction performance might not be great because midterms took priority
# Load the test dataset
test_data <- read.csv("Prediction2025-2TestStud.csv")
# Encode categorical variables in the test data
# (ensure that encoding is consistent with the training data)
test_data$Groom_MB <- as.numeric(factor(test_data$Groom_MB))
test_data$Bride_MB <- as.numeric(factor(test_data$Bride_MB))
test_data$Groom_Edu <- as.numeric(factor(test_data$Groom_Edu))
test_data$Bride_Edu <- as.numeric(factor(test_data$Bride_Edu))
# Use the trained decision tree to predict outcomes for the test data
test_pred <- predict(tree, newdata = test_data, type = "class")
# Convert numeric predictions back to character labels if needed
# Here, assuming 1 represents "Success" and 0 represents "Failure"
test_data$Outcome <- ifelse(test_pred == 1, "Success", "Failure")
# Write the predictions to a CSV file for submission
write.csv(test_data, file = "submission.csv", row.names = FALSE)
# Optional: Print a message to confirm that the submission file has been created.
print("Submission file 'submission.csv' has been created successfully.")
library(rpart)
library(rpart.plot)
# Load the dataset
data <- read.csv("Prediction2025-2Train.csv")
# Encode categorical variables in R
data$Groom_MB <- as.numeric(factor(data$Groom_MB))
data$Bride_MB <- as.numeric(factor(data$Bride_MB))
data$Groom_Edu <- as.numeric(factor(data$Groom_Edu))
data$Bride_Edu <- as.numeric(factor(data$Bride_Edu))
data$Outcome <- ifelse(data$Outcome == "Success", 1, 0)
# Train a more flexible decision tree
tree <- rpart(Outcome ~ ., data = data, method = "class",
cp = 0.001,     # Reduce pruning for better learning
minsplit = 5,   # Allow smaller nodes to split
minbucket = 2,  # Reduce required instances per leaf
maxdepth = 10)  # Allow deeper trees for better accuracy
# Predictions
pred <- predict(tree, data, type = "class")
data$predict <- pred
# Evaluate accuracy
data$predict <- ifelse(data$predict == 1, "Success", "Failure")
accuracy <- mean(data$Outcome == data$predict)
print(paste("Model Accuracy:", round(accuracy, 4)))
# Visualize the improved tree
rpart.plot(tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Improved Decision Tree")
# Note: Prediction performance might not be great because midterms took priority
# Load the test dataset
test_data <- read.csv("Prediction2025-2TestStud.csv")
# Encode categorical variables in the test data
# (ensure that encoding is consistent with the training data)
test_data$Groom_MB <- as.numeric(factor(test_data$Groom_MB))
test_data$Bride_MB <- as.numeric(factor(test_data$Bride_MB))
test_data$Groom_Edu <- as.numeric(factor(test_data$Groom_Edu))
test_data$Bride_Edu <- as.numeric(factor(test_data$Bride_Edu))
# Use the trained decision tree to predict outcomes for the test data
test_pred <- predict(tree, newdata = test_data, type = "class")
# Convert numeric predictions back to character labels if needed
# Here, assuming 1 represents "Success" and 0 represents "Failure"
test_data$Outcome <- ifelse(test_pred == 1, "Success", "Failure")
# Write the predictions to a CSV file for submission
write.csv(test_data, file = "submission.csv", row.names = FALSE)
# Optional: Print a message to confirm that the submission file has been created.
print("Submission file 'submission.csv' has been created successfully.")
library(rpart)
library(rpart.plot)
# Load the dataset
data <- read.csv("Prediction2025-2Train.csv")
# Encode categorical variables in R
data$Groom_MB <- as.numeric(factor(data$Groom_MB))
data$Bride_MB <- as.numeric(factor(data$Bride_MB))
data$Groom_Edu <- as.numeric(factor(data$Groom_Edu))
data$Bride_Edu <- as.numeric(factor(data$Bride_Edu))
data$Outcome <- ifelse(data$Outcome == "Success", 1, 0)
# Train a more flexible decision tree
tree <- rpart(Outcome ~ ., data = data, method = "class",
cp = 0.001,     # Reduce pruning for better learning
minsplit = 5,   # Allow smaller nodes to split
minbucket = 2,  # Reduce required instances per leaf
maxdepth = 10)  # Allow deeper trees for better accuracy
# Predictions
pred <- predict(tree, data, type = "class")
data$predict <- pred
# Evaluate accuracy
test$predict <- ifelse(data$predict == 1, "Success", "Failure")
accuracy <- mean(data$Outcome == data$predict)
print(paste("Model Accuracy:", round(accuracy, 4)))
# Visualize the improved tree
rpart.plot(tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Improved Decision Tree")
data
library(rpart)
library(rpart.plot)
# Load the dataset
data <- read.csv("Prediction2025-2Train.csv")
# Encode categorical variables in R
data$Groom_MB <- as.numeric(factor(data$Groom_MB))
data$Bride_MB <- as.numeric(factor(data$Bride_MB))
data$Groom_Edu <- as.numeric(factor(data$Groom_Edu))
data$Bride_Edu <- as.numeric(factor(data$Bride_Edu))
data$Outcome <- ifelse(data$Outcome == "Success", 1, 0)
# Train a more flexible decision tree
tree <- rpart(Outcome ~ ., data = data, method = "class",
cp = 0.001,     # Reduce pruning for better learning
minsplit = 5,   # Allow smaller nodes to split
minbucket = 2,  # Reduce required instances per leaf
maxdepth = 10)  # Allow deeper trees for better accuracy
# Predictions
pred <- predict(tree, data, type = "class")
data$predict <- pred
# Evaluate accuracy
accuracy <- mean(data$Outcome == data$predict)
print(paste("Model Accuracy:", round(accuracy, 4)))
# Visualize the improved tree
rpart.plot(tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Improved Decision Tree")
# Load the test dataset
test_data <- read.csv("Prediction2025-2TestStud.csv")
# Encode categorical variables in the test data
# (ensure that encoding is consistent with the training data)
test_data$Groom_MB <- as.numeric(factor(test_data$Groom_MB))
test_data$Bride_MB <- as.numeric(factor(test_data$Bride_MB))
test_data$Groom_Edu <- as.numeric(factor(test_data$Groom_Edu))
test_data$Bride_Edu <- as.numeric(factor(test_data$Bride_Edu))
# Use the trained decision tree to predict outcomes for the test data
test_pred <- predict(tree, newdata = test_data, type = "class")
# Convert numeric predictions back to character labels if needed
# Here, assuming 1 represents "Success" and 0 represents "Failure"
test_data$Outcome <- ifelse(test_pred == 1, "Success", "Failure")
# Write the predictions to a CSV file for submission
submission <- read.csv("Prediction2025-2submission.csv")
submission$Outcome <- test_data$Outcome
write.csv(submission, file = "Prediction2025-2submission.csv", row.names = FALSE)
